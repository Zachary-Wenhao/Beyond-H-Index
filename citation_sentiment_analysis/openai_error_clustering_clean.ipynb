{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a089c7",
   "metadata": {},
   "source": [
    "# OpenAI Error Clustering - Clean Pipeline\n",
    "\n",
    "This notebook performs automated clustering of ALL citation sentiment analysis errors using OpenAI's API.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load ALL errors from CSV\n",
    "2. Use OpenAI to extract linguistic cues and citation intents\n",
    "3. Cluster errors by AI-identified patterns\n",
    "4. Export results for analysis\n",
    "\n",
    "No over-visualization - just clean, systematic clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b21f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found API key in environment: sk-proj-0Y1BzM6...\n",
      "âœ… OpenAI client initialized\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Use the API key from terminal environment\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"âŒ No API key found in environment!\")\n",
    "    print(\"Please run: export OPENAI_API_KEY='your-key-here'\")\n",
    "else:\n",
    "    print(f\"âœ… Found API key in environment: {api_key[:15]}...\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "print(\"âœ… OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc0ea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading error analysis data...\n",
      "Total samples: 1596\n",
      "\n",
      "ğŸ¯ PROCESSING ALL 239 ERRORS\n",
      "Error rate: 15.0%\n",
      "\n",
      "Error type distribution:\n",
      "  o â†’ p: 98 (41.0%)\n",
      "  o â†’ n: 58 (24.3%)\n",
      "  p â†’ o: 48 (20.1%)\n",
      "  n â†’ o: 17 (7.1%)\n",
      "  p â†’ n: 13 (5.4%)\n",
      "  n â†’ p: 5 (2.1%)\n",
      "\n",
      "âœ… Ready to analyze ALL 239 errors with OpenAI\n"
     ]
    }
   ],
   "source": [
    "# Load ALL errors from CSV\n",
    "print(\"ğŸ“‚ Loading error analysis data...\")\n",
    "\n",
    "df = pd.read_csv('error_analysis_results.csv')\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "\n",
    "# Filter for errors only (misclassified samples)\n",
    "errors_df = df[df['label'] != df['predicted_label']].copy()\n",
    "total_errors = len(errors_df)\n",
    "\n",
    "print(f\"\\nğŸ¯ PROCESSING ALL {total_errors} ERRORS\")\n",
    "print(f\"Error rate: {total_errors / len(df) * 100:.1f}%\")\n",
    "\n",
    "# Add error type column\n",
    "errors_df['error_type'] = errors_df.apply(lambda row: f\"{row['label']} â†’ {row['predicted_label']}\", axis=1)\n",
    "\n",
    "print(f\"\\nError type distribution:\")\n",
    "error_type_counts = errors_df['error_type'].value_counts()\n",
    "for error_type, count in error_type_counts.items():\n",
    "    print(f\"  {error_type}: {count} ({count/total_errors*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Ready to analyze ALL {total_errors} errors with OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "543043e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "ğŸ” Test 1: This approach builds on the work of Smith et al. (2020) to improve performance.\n",
      "====================\n",
      "âŒ An error occurred during the API call or parsing: '\\n  \"reasoning\"'\n",
      "\n",
      "âŒ Failed to get a result.\n",
      "\n",
      "====================\n",
      "ğŸ” Test 2: However, the method proposed by Jones (2019) fails to achieve adequate results.\n",
      "====================\n",
      "âŒ An error occurred during the API call or parsing: '\\n  \"reasoning\"'\n",
      "\n",
      "âŒ Failed to get a result.\n",
      "\n",
      "====================\n",
      "ğŸ” Test 3: We compare our BLEU scores against those reported in previous studies (Doe et al., 2021), but our approach might be less generalizable.\n",
      "====================\n",
      "âŒ An error occurred during the API call or parsing: '\\n  \"reasoning\"'\n",
      "\n",
      "âŒ Failed to get a result.\n"
     ]
    }
   ],
   "source": [
    "ANALYSIS_PROMPT_COT = \"\"\"You are an expert linguistic analyst. Your task is to carefully analyze a citation sentence by first reasoning about its properties and then providing a final classification.\n",
    "\n",
    "Follow these two steps precisely:\n",
    "\n",
    "**Step 1: Chain-of-Thought Reasoning**\n",
    "First, provide a step-by-step analysis inside a `reasoning` JSON object. For each category, briefly explain your decision with evidence from the text.\n",
    "- Analyze each surface-level cue (hedging, contrastive, etc.) one by one.\n",
    "- Conclude with your reasoning for the final citation `intent`.\n",
    "\n",
    "**Step 2: Final Classification**\n",
    "After your reasoning, provide the final classification inside a `classification` JSON object, following the required 0/1 format and intent string.\n",
    "\n",
    "Return a single, top-level JSON object with exactly two keys: \"reasoning\" and \"classification\".\n",
    "\n",
    "**EXAMPLE OUTPUT STRUCTURE:**\n",
    "{\n",
    "  \"reasoning\": {\n",
    "    \"hedging\": \"No hedging words like 'may' or 'seems' are present.\",\n",
    "    \"contrastive\": \"The word 'However' is a clear contrastive marker, indicating a shift in argument.\",\n",
    "    \"negative_eval\": \"The phrase 'fails to achieve' is direct criticism.\",\n",
    "    \"intent\": \"The sentence uses a contrastive word and negative evaluation to point out a flaw in prior work. Therefore, the intent is 'Weakness/Limitation'.\"\n",
    "  },\n",
    "  \"classification\": {\n",
    "    \"hedging\": 0,\n",
    "    \"contrastive\": 1,\n",
    "    \"positive_eval\": 0,\n",
    "    \"negative_eval\": 1,\n",
    "    \"metric\": 0,\n",
    "    \"multi_citation\": 0,\n",
    "    \"intent\": \"Weakness/Limitation\"\n",
    "  }\n",
    "}\n",
    "\n",
    "---\n",
    "\n",
    "Now, analyze this citation context:\n",
    "\"{CITATION_SENTENCE}\"\n",
    "\"\"\"\n",
    "\n",
    "# The Updated Python Function\n",
    "def analyze_citation_with_cot(text: str) -> Optional[Dict]:\n",
    "    \"\"\"Analyze citation using a Chain-of-Thought prompt for higher accuracy.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",  # Using a more capable model is recommended for CoT\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output a structured JSON object containing your reasoning and final classification.\"},\n",
    "                {\"role\": \"user\", \"content\": ANALYSIS_PROMPT_COT.format(CITATION_SENTENCE=text)}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        full_result = json.loads(content)\n",
    "        \n",
    "        print(\"ğŸ§  Model Reasoning:\")\n",
    "        reasoning = full_result.get(\"reasoning\", {})\n",
    "        for key, value in reasoning.items():\n",
    "            print(f\"   - {key.title()}: {value}\")\n",
    "        \n",
    "        classification = full_result.get(\"classification\", {})\n",
    "        \n",
    "        clean_result = {\n",
    "            'hedging': int(classification.get('hedging', 0)),\n",
    "            'contrastive': int(classification.get('contrastive', 0)),\n",
    "            'positive_eval': int(classification.get('positive_eval', 0)),\n",
    "            'negative_eval': int(classification.get('negative_eval', 0)),\n",
    "            'metric': int(classification.get('metric', 0)),\n",
    "            'multi_citation': int(classification.get('multi_citation', 0)),\n",
    "            'intent': str(classification.get('intent', 'Other'))\n",
    "        }\n",
    "        return clean_result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ An error occurred during the API call or parsing: {e}\")\n",
    "        return None\n",
    "\n",
    "# The Test Harness\n",
    "test_samples = [\n",
    "    \"This approach builds on the work of Smith et al. (2020) to improve performance.\",\n",
    "    \"However, the method proposed by Jones (2019) fails to achieve adequate results.\",\n",
    "    \"We compare our BLEU scores against those reported in previous studies (Doe et al., 2021), but our approach might be less generalizable.\"\n",
    "]\n",
    "\n",
    "for i, sample in enumerate(test_samples):\n",
    "    print(f\"\\n{'='*20}\\nğŸ” Test {i+1}: {sample}\\n{'='*20}\")\n",
    "    result = analyze_citation_with_cot(sample)\n",
    "    if result:\n",
    "        print(f\"\\nâœ… Final Classification: {json.dumps(result, indent=2)}\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Failed to get a result.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e9fa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ PROCESSING ALL 239 ERROR CITATIONS\n",
      "â±ï¸ Estimated time: 8.0 minutes\n",
      "ğŸ’° Estimated cost: ~$0.48\n",
      "ğŸ“Š Processing 1/239 | Success: 0 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 20/239 | Success: 4 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 160/239 | Success: 27 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 180/239 | Success: 30 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 300/239 | Success: 50 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 350/239 | Success: 57 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 400/239 | Success: 62 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 440/239 | Success: 75 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 480/239 | Success: 78 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 500/239 | Success: 84 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 550/239 | Success: 93 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 570/239 | Success: 97 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 680/239 | Success: 112 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 840/239 | Success: 127 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 940/239 | Success: 142 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 1050/239 | Success: 160 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 1210/239 | Success: 181 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 1260/239 | Success: 184 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 1270/239 | Success: 188 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 1320/239 | Success: 197 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 1330/239 | Success: 198 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 1350/239 | Success: 203 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 1370/239 | Success: 207 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 1390/239 | Success: 210 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 1560/239 | Success: 231 | Failed: 0 | Consecutive fails: 0\n",
      "ğŸ“Š Processing 1590/239 | Success: 237 | Failed: 0 | Consecutive fails: 0\n",
      "\n",
      "âœ… BATCH PROCESSING COMPLETE!\n",
      "ğŸ“Š Results: 239 successful, 0 failed\n",
      "ğŸ“ˆ Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Batch process ALL errors with improved error handling\n",
    "print(f\"ğŸš€ PROCESSING ALL {len(errors_df)} ERROR CITATIONS\")\n",
    "print(f\"â±ï¸ Estimated time: {len(errors_df) * 2.0 / 60:.1f} minutes\")  # Increased time estimate\n",
    "print(f\"ğŸ’° Estimated cost: ~${len(errors_df) * 0.002:.2f}\")\n",
    "\n",
    "# Initialize result columns\n",
    "errors_df['api_hedging'] = None\n",
    "errors_df['api_contrastive'] = None\n",
    "errors_df['api_positive_eval'] = None\n",
    "errors_df['api_negative_eval'] = None\n",
    "errors_df['api_metric'] = None\n",
    "errors_df['api_multi_citation'] = None\n",
    "errors_df['api_intent'] = None\n",
    "errors_df['api_success'] = False\n",
    "\n",
    "# Process each citation with retry logic\n",
    "successful = 0\n",
    "failed = 0\n",
    "consecutive_failures = 0\n",
    "\n",
    "for idx, row in errors_df.iterrows():\n",
    "    text = row['text']\n",
    "    \n",
    "    # Show progress\n",
    "    current = idx - errors_df.index[0] + 1\n",
    "    total = len(errors_df)\n",
    "    \n",
    "    if current % 10 == 0 or current == 1:  # More frequent updates\n",
    "        print(f\"ğŸ“Š Processing {current}/{total} | Success: {successful} | Failed: {failed} | Consecutive fails: {consecutive_failures}\")\n",
    "    \n",
    "    # Try to analyze citation with retry\n",
    "    result = None\n",
    "    max_retries = 2\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        result = analyze_citation(text)\n",
    "        if result:\n",
    "            break\n",
    "        elif attempt < max_retries - 1:\n",
    "            print(f\"  ğŸ”„ Retry {attempt + 2} for citation {current}\")\n",
    "            time.sleep(2)  # Longer delay before retry\n",
    "    \n",
    "    if result:\n",
    "        # Store results\n",
    "        errors_df.at[idx, 'api_hedging'] = result.get('hedging', 0)\n",
    "        errors_df.at[idx, 'api_contrastive'] = result.get('contrastive', 0)\n",
    "        errors_df.at[idx, 'api_positive_eval'] = result.get('positive_eval', 0)\n",
    "        errors_df.at[idx, 'api_negative_eval'] = result.get('negative_eval', 0)\n",
    "        errors_df.at[idx, 'api_metric'] = result.get('metric', 0)\n",
    "        errors_df.at[idx, 'api_multi_citation'] = result.get('multi_citation', 0)\n",
    "        errors_df.at[idx, 'api_intent'] = result.get('intent', 'Other/Unclear')\n",
    "        errors_df.at[idx, 'api_success'] = True\n",
    "        successful += 1\n",
    "        consecutive_failures = 0\n",
    "    else:\n",
    "        failed += 1\n",
    "        consecutive_failures += 1\n",
    "        \n",
    "        # If too many consecutive failures, pause longer\n",
    "        if consecutive_failures >= 5:\n",
    "            print(f\"âš ï¸ {consecutive_failures} consecutive failures, pausing 10 seconds...\")\n",
    "            time.sleep(10)\n",
    "            consecutive_failures = 0\n",
    "    \n",
    "    # Rate limiting - longer delays due to errors\n",
    "    time.sleep(1.5)\n",
    "    \n",
    "    # Emergency break if failure rate is too high\n",
    "    if current >= 20 and failed / current > 0.8:\n",
    "        print(f\"âŒ STOPPING: Failure rate too high ({failed}/{current} = {failed/current*100:.1f}%)\")\n",
    "        print(\"Check API key, rate limits, or prompt format\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ… BATCH PROCESSING COMPLETE!\")\n",
    "print(f\"ğŸ“Š Results: {successful} successful, {failed} failed\")\n",
    "if successful + failed > 0:\n",
    "    print(f\"ğŸ“ˆ Success rate: {successful / (successful + failed) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a45061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ANALYZING 239 SUCCESSFUL RESULTS\n",
      "\n",
      "ğŸ”§ SURFACE CUE DISTRIBUTION:\n",
      "  Hedging        :  22/239 ( 9.2%)\n",
      "  Contrastive    :  38/239 (15.9%)\n",
      "  Positive Eval  :  14/239 ( 5.9%)\n",
      "  Negative Eval  :  11/239 ( 4.6%)\n",
      "  Metric         :  12/239 ( 5.0%)\n",
      "  Multi-Citation : 202/239 (84.5%)\n",
      "\n",
      "ğŸ¯ INTENT DISTRIBUTION:\n",
      "  Background          : 239/239 (100.0%)\n",
      "\n",
      "âŒ ERROR TYPES BY INTENT:\n",
      "\n",
      "  ğŸ“‚ Background (239 samples):\n",
      "    o â†’ p               : 98 ( 41%)\n",
      "    o â†’ n               : 58 ( 24%)\n",
      "    p â†’ o               : 48 ( 20%)\n",
      "\n",
      "âœ… Analysis complete for 239 citations\n"
     ]
    }
   ],
   "source": [
    "# Analyze results - focus on successful analyses\n",
    "successful_results = errors_df[errors_df['api_success'] == True].copy()\n",
    "print(f\"ğŸ“Š ANALYZING {len(successful_results)} SUCCESSFUL RESULTS\")\n",
    "\n",
    "if len(successful_results) == 0:\n",
    "    print(\"âŒ No successful results to analyze\")\n",
    "else:\n",
    "    # Surface cue distribution\n",
    "    print(f\"\\nğŸ”§ SURFACE CUE DISTRIBUTION:\")\n",
    "    cue_columns = ['api_hedging', 'api_contrastive', 'api_positive_eval', 'api_negative_eval', 'api_metric', 'api_multi_citation']\n",
    "    cue_names = ['Hedging', 'Contrastive', 'Positive Eval', 'Negative Eval', 'Metric', 'Multi-Citation']\n",
    "    \n",
    "    for col, name in zip(cue_columns, cue_names):\n",
    "        count = successful_results[col].sum()\n",
    "        pct = count / len(successful_results) * 100\n",
    "        print(f\"  {name:15}: {count:3d}/{len(successful_results)} ({pct:4.1f}%)\")\n",
    "    \n",
    "    # Intent distribution\n",
    "    print(f\"\\nğŸ¯ INTENT DISTRIBUTION:\")\n",
    "    intent_counts = successful_results['api_intent'].value_counts()\n",
    "    for intent, count in intent_counts.items():\n",
    "        pct = count / len(successful_results) * 100\n",
    "        print(f\"  {intent:20}: {count:3d}/{len(successful_results)} ({pct:4.1f}%)\")\n",
    "    \n",
    "    # Error type breakdown by intent\n",
    "    print(f\"\\nâŒ ERROR TYPES BY INTENT:\")\n",
    "    for intent in intent_counts.index[:3]:  # Top 3 intents\n",
    "        intent_data = successful_results[successful_results['api_intent'] == intent]\n",
    "        error_dist = intent_data['error_type'].value_counts()\n",
    "        print(f\"\\n  ğŸ“‚ {intent} ({len(intent_data)} samples):\")\n",
    "        for error_type, count in error_dist.head(3).items():\n",
    "            pct = count / len(intent_data) * 100\n",
    "            print(f\"    {error_type:20}: {count:2d} ({pct:3.0f}%)\")\n",
    "    \n",
    "    print(f\"\\nâœ… Analysis complete for {len(successful_results)} citations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c69ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clustering signatures and export results\n",
    "if len(successful_results) > 0:\n",
    "    print(\"ğŸ”§ CREATING CLUSTERING SIGNATURES...\")\n",
    "    \n",
    "    # Create surface cue signature\n",
    "    def create_cue_signature(row):\n",
    "        cues = []\n",
    "        if row['api_hedging']: cues.append('H')\n",
    "        if row['api_contrastive']: cues.append('C')\n",
    "        if row['api_positive_eval']: cues.append('P+')\n",
    "        if row['api_negative_eval']: cues.append('P-')\n",
    "        if row['api_metric']: cues.append('M')\n",
    "        if row['api_multi_citation']: cues.append('MC')\n",
    "        return '_'.join(cues) if cues else 'NONE'\n",
    "    \n",
    "    successful_results['cue_signature'] = successful_results.apply(create_cue_signature, axis=1)\n",
    "    successful_results['cluster_label'] = successful_results['api_intent'] + '_' + successful_results['cue_signature']\n",
    "    \n",
    "    # Show top clusters\n",
    "    print(f\"\\nğŸ“‚ TOP ERROR CLUSTERS:\")\n",
    "    cluster_counts = successful_results['cluster_label'].value_counts().head(10)\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        pct = count / len(successful_results) * 100\n",
    "        print(f\"  {cluster:35}: {count:2d} ({pct:3.1f}%)\")\n",
    "    \n",
    "    # Export main results\n",
    "    export_columns = [\n",
    "        'text', 'label', 'predicted_label', 'error_type',\n",
    "        'api_hedging', 'api_contrastive', 'api_positive_eval', 'api_negative_eval',\n",
    "        'api_metric', 'api_multi_citation', 'api_intent',\n",
    "        'cue_signature', 'cluster_label', 'api_success'\n",
    "    ]\n",
    "    \n",
    "    if 'confidence' in successful_results.columns:\n",
    "        export_columns.insert(4, 'confidence')\n",
    "    \n",
    "    # Export to CSV\n",
    "    output_file = 'openai_clustered_all_errors.csv'\n",
    "    successful_results[export_columns].to_csv(output_file, index=False)\n",
    "    print(f\"\\nğŸ’¾ EXPORTED {len(successful_results)} clustered errors to: {output_file}\")\n",
    "    \n",
    "    # Create cluster summary\n",
    "    cluster_summary = []\n",
    "    for cluster_label in cluster_counts.index:\n",
    "        cluster_data = successful_results[successful_results['cluster_label'] == cluster_label]\n",
    "        \n",
    "        # Most common error type in this cluster\n",
    "        top_error = cluster_data['error_type'].value_counts().iloc[0]\n",
    "        top_error_type = cluster_data['error_type'].value_counts().index[0]\n",
    "        \n",
    "        cluster_summary.append({\n",
    "            'cluster_label': cluster_label,\n",
    "            'total_count': len(cluster_data),\n",
    "            'percentage_of_errors': len(cluster_data) / len(successful_results) * 100,\n",
    "            'top_error_type': top_error_type,\n",
    "            'top_error_count': top_error,\n",
    "            'example_text': cluster_data.iloc[0]['text'][:100] + \"...\"\n",
    "        })\n",
    "    \n",
    "    cluster_summary_df = pd.DataFrame(cluster_summary)\n",
    "    summary_file = 'cluster_summary_all_errors.csv'\n",
    "    cluster_summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"ğŸ’¾ EXPORTED cluster summary to: {summary_file}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ CLUSTERING COMPLETE!\")\n",
    "    print(f\"ğŸ“Š Processed: {len(errors_df)} total errors\")\n",
    "    print(f\"âœ… Successful: {len(successful_results)} analyzed citations\")\n",
    "    print(f\"ğŸ“‚ Clusters: {len(cluster_counts)} unique error patterns identified\")\n",
    "    print(f\"ğŸ’¾ Files: {output_file}, {summary_file}\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No successful results to cluster\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
